{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "surprising-victoria",
   "metadata": {},
   "source": [
    "Here are used custom functions and classes, their code is found on Custom_Class_Func.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, plot_roc_curve\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('weatherAUS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-australia",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['RainTomorrow'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['RainTomorrow'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-custom",
   "metadata": {},
   "source": [
    "Dataset is imbalanced and there are several missing values both in the X features and between the target values. We will evaluate the model in the following different dataset configurations:\n",
    "- Without the examples relative to the target missing values, with the X missing values handled (imputed) by xgboost;\n",
    "- With all the missing values imputed by kNN Imputer from sklearn.\n",
    "\n",
    "We will also handle the imbalance by:\n",
    "- Stratifying;\n",
    "- Ensembling the resampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = data.isna().sum().sort_values()\n",
    "missing_ratio = data.isna().sum()/data.isna().count()\n",
    "missing_matrix = pd.concat([missing, missing_ratio], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the features with more than one third of data missing\n",
    "\n",
    "drop_indexes = missing_matrix[missing_matrix[1] > 0.33].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = data.drop(drop_indexes, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-formation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only data with target values not missing\n",
    "X_y = processed_data[processed_data['RainTomorrow'].isna() == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-darwin",
   "metadata": {},
   "source": [
    "Testing with chi squared and anova to see if some features can be discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = X_y[['Location','WindGustDir','WindDir9am','WindDir3pm','RainToday','RainTomorrow']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-fraud",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "cat = cat.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_scores = chi2(cat.drop(['RainTomorrow'], axis=1), cat['RainTomorrow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = data.drop(['Location','WindGustDir','WindDir9am','WindDir3pm','RainToday','RainTomorrow'], axis=1)\n",
    "cont['RainTomorrow'] = cat['RainTomorrow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont.dropna(inplace=True)\n",
    "cont.drop('Date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-oliver",
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_scores = f_classif(cont.drop(['RainTomorrow'], axis=1), cont['RainTomorrow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = pd.Series(chi_scores[1],index = cat.columns[0:-1])\n",
    "p_values.plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_p_values = pd.Series(anova_scores[1],index = cont.columns[0:-1])\n",
    "f_p_values.plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-bowling",
   "metadata": {},
   "source": [
    "The test's p-values indicate that location must be discarded since its presence it's not statistically significant if we choose alpha = 0.05. Despite of this, location can be important since in certain areas some conditions could give rain while in other not. Furthermore alpha can be choosen higher since Type I errors do not lead to dramatic effects in this case (e.g. counting location when it's not needed). For the sake of curiosity, for once, we will produce also the results relative to not taking location into account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-legislation",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_y.drop('RainTomorrow', axis=1)\n",
    "y = X_y['RainTomorrow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-strategy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Custom_Class_Func import OneHotEncoder_with_NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-constitution",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = OneHotEncoder_with_NaNs(['Location','WindGustDir','WindDir9am',\n",
    "                            'WindDir3pm','RainToday']).fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop('Date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-latex",
   "metadata": {},
   "source": [
    "# No Imputing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-range",
   "metadata": {},
   "source": [
    "## Stratifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=True, stratify=y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use n_jobs to parallelize computation among different threads \n",
    "# (in my pc max 12, but empiric evidence shows best with 10)\n",
    "# we also will use early stopping to avoid overfitting\n",
    "\n",
    "model = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, n_estimators = 10000, n_jobs=10)\n",
    "model.fit(X_train, y_train, verbose=False,\n",
    "            early_stopping_rounds=10, eval_metric='auc', eval_set=[(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-longer",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "results = pd.Series({'AUC_ROC' : roc_auc_score(y_test, y_pred), \n",
    "                     'Precision' : precision_score(y_test, y_pred),\n",
    "                     'Recall' : recall_score(y_test, y_pred)},\n",
    "                      name='No_imput_Stratify')\n",
    "                                \n",
    "print(confusion_matrix(y_test, y_pred)/y_pred.shape, '\\n')\n",
    "print('Area under ROC curve:', results['AUC_ROC'])\n",
    "print('Precision:',results['Precision'])\n",
    "print('Recall:', results['Recall'])\n",
    "plot_roc_curve(model, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-cooling",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine tuning, taking imbalance in account\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.1, 0.01, 0.05],\n",
    "    'gamma': [0, 0.25, 1.0],\n",
    "    'reg_lambda': [0, 1.0, 10.0],\n",
    "    'scale_pos_weight': [3, 5, 44] # XGBoost recommends: sum(negative instances) / sum(positive instances)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-tobacco",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_params = GridSearchCV(\n",
    "    estimator=xgb.XGBClassifier(objective='binary:logistic', n_estimators = 10000,\n",
    "                               subsample=0.9, colsample_bytree=0.5),\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    verbose=False,\n",
    "    n_jobs = 10,\n",
    "    cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_params.fit(X_train, \n",
    "                   y_train, \n",
    "                   early_stopping_rounds=10,                \n",
    "                   eval_metric='auc',\n",
    "                   eval_set=[(X_test, y_test)],\n",
    "                   verbose=False)\n",
    "print(optimal_params.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run again tuning with borderline params from before\n",
    "param_grid = {\n",
    "    'max_depth': [5],\n",
    "    'learning_rate': [0.05],\n",
    "    'gamma': [0.25],\n",
    "    'reg_lambda': [10.0],\n",
    "    'scale_pos_weight': [2,3] \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-upper",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_params = GridSearchCV(\n",
    "    estimator=xgb.XGBClassifier(objective='binary:logistic', n_estimators = 10000,\n",
    "                               subsample=0.9, colsample_bytree=0.5),\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    verbose=False,\n",
    "    n_jobs = 10,\n",
    "    cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_params.fit(X_train, \n",
    "                   y_train, \n",
    "                   early_stopping_rounds=10,                \n",
    "                   eval_metric='auc',\n",
    "                   eval_set=[(X_test, y_test)],\n",
    "                   verbose=False)\n",
    "print(optimal_params.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The previous params are confirmed\n",
    "model = xgb.XGBClassifier(objective='binary:logistic', gamma=0.25, learn_rate=0.05,\n",
    "                        max_depth=5, reg_lambda=10, scale_pos_weight=3, n_estimators=10000, n_jobs=10)\n",
    "model.fit(X_train, y_train, verbose=False,\n",
    "            early_stopping_rounds=10, eval_metric='auc', eval_set=[(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-attachment",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "a = {'AUC_ROC' : roc_auc_score(y_test, y_pred), 'Precision' : precision_score(y_test, y_pred),\n",
    "        'Recall' : recall_score(y_test, y_pred)}\n",
    "\n",
    "results = pd.concat([results, pd.Series(a, name='No_imput_Stratify_optimized')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred)/y_pred.shape, '\\n')\n",
    "print('Area under Roc curve:', a['AUC_ROC'])\n",
    "print('Precision:',a['Precision'])\n",
    "print('Recall:', a['Recall'])\n",
    "plot_roc_curve(model, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-heart",
   "metadata": {},
   "source": [
    "## Ensembled resampling\n",
    "We will use **custom functions** to compute predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Custom_Class_Func import get_resampled_datasets, get_xgboost_ensemble_predictions, ensemble_xgboost_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-booth",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ensemble_xgboost_CV(pd.concat([X,y], axis=1), threads=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([results, pd.Series(a, name='No_imput_Ensembled')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-pioneer",
   "metadata": {},
   "source": [
    "# kNN Imputer\n",
    "## Stratifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the dataset with missing values in target values\n",
    "X_y = processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_y.drop('RainTomorrow', axis=1)\n",
    "y = X_y['RainTomorrow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = OneHotEncoder_with_NaNs(['Location','WindGustDir','WindDir9am',\n",
    "                            'WindDir3pm','RainToday']).fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('Date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=4)\n",
    "imputed_dataset = imputer.fit_transform(pd.concat([X,y], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-classic",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = pd.concat([X,y], axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-marina",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y = pd.DataFrame(imputed_dataset, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_y.drop('RainTomorrow', axis=1)\n",
    "y = X_y['RainTomorrow']\n",
    "\n",
    "# Because imputer sometimes gives values between 1 and 0 and the majority of y values are 0:\n",
    "y = y.apply(lambda x: 0 if x <= 0.5 else 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=True, stratify=y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, n_estimators = 10000, n_jobs=10)\n",
    "model_1.fit(X_train, y_train, verbose=False,\n",
    "            early_stopping_rounds=10, eval_metric='auc', eval_set=[(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_1.predict(X_test)\n",
    "a = {'AUC_ROC' : roc_auc_score(y_test, y_pred), 'Precision' : precision_score(y_test, y_pred),\n",
    "        'Recall' : recall_score(y_test, y_pred)}\n",
    "\n",
    "results = pd.concat([results, pd.Series(a, name='kNNImput_Stratify')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-cookbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred)/y_pred.shape)\n",
    "print('Area under Roc curve:', roc_auc_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))\n",
    "print('Recall:', recall_score(y_test, y_pred))\n",
    "plot_roc_curve(model_1, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-establishment",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [5],\n",
    "    'learning_rate': [0.05],\n",
    "    'gamma': [0, 0.25, 1.0],\n",
    "    'reg_lambda': [10.0],\n",
    "    'scale_pos_weight': [2, 3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_params = GridSearchCV(\n",
    "    estimator=xgb.XGBClassifier(objective='binary:logistic', n_estimators = 10000,\n",
    "                               subsample=0.9, colsample_bytree=0.5),\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    verbose=False,\n",
    "    n_jobs = 10,\n",
    "    cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-heart",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimal_params.fit(X_train, \n",
    "                   y_train, \n",
    "                   early_stopping_rounds=10,                \n",
    "                   eval_metric='auc',\n",
    "                   eval_set=[(X_test, y_test)],\n",
    "                   verbose=False)\n",
    "print(optimal_params.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The previous params are confirmed\n",
    "model_1 = xgb.XGBClassifier(objective='binary:logistic', gamma=0.25, learn_rate=0.05,\n",
    "                        max_depth=5, reg_lambda=10, scale_pos_weight=2, n_estimators=10000, n_jobs=10)\n",
    "model_1.fit(X_train, y_train, verbose=False,\n",
    "            early_stopping_rounds=10, eval_metric='auc', eval_set=[(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_1.predict(X_test)\n",
    "a = {'AUC_ROC' : roc_auc_score(y_test, y_pred), 'Precision' : precision_score(y_test, y_pred),\n",
    "        'Recall' : recall_score(y_test, y_pred)}\n",
    "\n",
    "results = pd.concat([results, pd.Series(a, name='kNNImput_Stratify_optimized')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred)/y_pred.shape)\n",
    "print('Area under Roc curve:', roc_auc_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))\n",
    "print('Recall:', recall_score(y_test, y_pred))\n",
    "plot_roc_curve(model_1, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-forum",
   "metadata": {},
   "source": [
    "## Ensambled Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ensemble_xgboost_CV(pd.concat([X,y], axis=1), threads=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([results, pd.Series(a, name='kNNImput_Ensembled')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-tactics",
   "metadata": {},
   "source": [
    "# Confrontation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-assurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-violin",
   "metadata": {},
   "source": [
    "# Final Considerations \n",
    "Stratifying per se yelds worse perfomance in terms of area under the ROC curve than ensembled resampling, while, through parameter optimization, it gives the best results overall.\\\n",
    "Leaving the missing values handling to xgboost makes no particular difference with imputing them with kNN. The latter performs slightly better, but the fact that some values have been imputed by an algorithm could be easily exploited by xgboost since imputation does not come from chance.\\\n",
    "Generally speaking, Ensembled samples, as a way to fight imbalance, give good results but are more prone to enhance recall rater than precision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
